{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Prepare climate FEVER dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# if you have pyarrow or fastparquet installed, pandas will pick one automatically\n",
    "climate_fever = pd.read_parquet(\"climate_fever/test-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_label</th>\n",
       "      <th>evidences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Global warming is driving polar bears toward e...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'evidence_id': 'Extinction risk from global ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The sun has gone into ‘lockdown’ which could c...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'evidence_id': 'Famine:386', 'evidence_label...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>The polar bear population has been growing.</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'evidence_id': 'Polar bear:1332', 'evidence_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Ironic' study finds more CO2 has slightly cool...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'evidence_id': 'Atmosphere of Mars:131', 'ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Human additions of CO2 are in the margin of er...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'evidence_id': 'Carbon dioxide in Earth's at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>They tell us that we are the primary forces co...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'evidence_id': 'Carbon dioxide:183', 'eviden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>The Great Barrier Reef is experiencing the mos...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'evidence_id': 'Coral bleaching:52', 'eviden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18</td>\n",
       "      <td>it’s not a pollutant that threatens human civi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'evidence_id': 'Air pollution:12', 'evidence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>If CO2 was so terrible for the planet, then in...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'evidence_id': 'Carbon dioxide in Earth's at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21</td>\n",
       "      <td>Sea level rise has been slow and a constant, p...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'evidence_id': 'Russia:153', 'evidence_label...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  claim_id                                              claim  claim_label  \\\n",
       "0        0  Global warming is driving polar bears toward e...            0   \n",
       "1        5  The sun has gone into ‘lockdown’ which could c...            0   \n",
       "2        6        The polar bear population has been growing.            1   \n",
       "3        9  Ironic' study finds more CO2 has slightly cool...            1   \n",
       "4       10  Human additions of CO2 are in the margin of er...            1   \n",
       "5       11  They tell us that we are the primary forces co...            0   \n",
       "6       14  The Great Barrier Reef is experiencing the mos...            0   \n",
       "7       18  it’s not a pollutant that threatens human civi...            1   \n",
       "8       19  If CO2 was so terrible for the planet, then in...            1   \n",
       "9       21  Sea level rise has been slow and a constant, p...            1   \n",
       "\n",
       "                                           evidences  \n",
       "0  [{'evidence_id': 'Extinction risk from global ...  \n",
       "1  [{'evidence_id': 'Famine:386', 'evidence_label...  \n",
       "2  [{'evidence_id': 'Polar bear:1332', 'evidence_...  \n",
       "3  [{'evidence_id': 'Atmosphere of Mars:131', 'ev...  \n",
       "4  [{'evidence_id': 'Carbon dioxide in Earth's at...  \n",
       "5  [{'evidence_id': 'Carbon dioxide:183', 'eviden...  \n",
       "6  [{'evidence_id': 'Coral bleaching:52', 'eviden...  \n",
       "7  [{'evidence_id': 'Air pollution:12', 'evidence...  \n",
       "8  [{'evidence_id': 'Carbon dioxide in Earth's at...  \n",
       "9  [{'evidence_id': 'Russia:153', 'evidence_label...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_fever.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclimate_fever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "print(climate_fever.to_dict().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'evidence_id': 'Extinction risk from global warming:170', 'evidence_label': 2, 'article': 'Extinction risk from global warming', 'evidence': '\"Recent Research Shows Human Activity Driving Earth Towards Global Extinction Event\".', 'entropy': 0.6931471824645996, 'votes': array(['SUPPORTS', 'NOT_ENOUGH_INFO', None, None, None], dtype=object)}\n",
      " {'evidence_id': 'Global warming:14', 'evidence_label': 0, 'article': 'Global warming', 'evidence': 'Environmental impacts include the extinction or relocation of many species as their ecosystems change, most immediately the environments of coral reefs, mountains, and the Arctic.', 'entropy': 0.0, 'votes': array(['SUPPORTS', 'SUPPORTS', None, None, None], dtype=object)}\n",
      " {'evidence_id': 'Global warming:178', 'evidence_label': 2, 'article': 'Global warming', 'evidence': 'Rising temperatures push bees to their physiological limits, and could cause the extinction of bee populations.', 'entropy': 0.6931471824645996, 'votes': array(['SUPPORTS', 'NOT_ENOUGH_INFO', None, None, None], dtype=object)}\n",
      " {'evidence_id': 'Habitat destruction:61', 'evidence_label': 0, 'article': 'Habitat destruction', 'evidence': 'Rising global temperatures, caused by the greenhouse effect, contribute to habitat destruction, endangering various species, such as the polar bear.', 'entropy': 0.0, 'votes': array(['SUPPORTS', 'SUPPORTS', None, None, None], dtype=object)}\n",
      " {'evidence_id': 'Polar bear:1328', 'evidence_label': 2, 'article': 'Polar bear', 'evidence': '\"Bear hunting caught in global warming debate\".', 'entropy': 0.6931471824645996, 'votes': array(['SUPPORTS', 'NOT_ENOUGH_INFO', None, None, None], dtype=object)}]\n"
     ]
    }
   ],
   "source": [
    "print(climate_fever.iloc[0]['evidences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def export_claims_with_supports(df, json_path):\n",
    "    \"\"\"\n",
    "    From a DataFrame `df` with columns ['claim_id','claim','evidences'...],\n",
    "    build a list of dicts containing only:\n",
    "      - claim_id\n",
    "      - claim\n",
    "      - evidences: [ { title, content, supports_pct }, … ]\n",
    "    and write it to `json_path` as pretty JSON.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for _, row in df.iterrows():\n",
    "        simple_evs = []\n",
    "        for ev in row[\"evidences\"]:\n",
    "            # remove None votes, calculate SUPPORTS pct\n",
    "            votes = [v for v in ev.get(\"votes\", []) if v is not None]\n",
    "            pct   = (votes.count(\"SUPPORTS\") / len(votes) * 100) if votes else 0\n",
    "            simple_evs.append({\n",
    "                \"title\":        ev[\"article\"],\n",
    "                \"content\":      ev[\"evidence\"],\n",
    "                \"supports_pct\": pct\n",
    "            })\n",
    "        output.append({\n",
    "            \"claim_id\":   row[\"claim_id\"],\n",
    "            \"claim\":      row[\"claim\"],\n",
    "            \"label\":      bool(row[\"claim_label\"]),\n",
    "            \"evidences\":  simple_evs\n",
    "        })\n",
    "\n",
    "    # write to JSON\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_claims_with_supports(climate_fever, \"prepared_datasets/climate_fever_with_evidence_details_labels.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Prepare COVIDFACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 1. Read the JSONL file (one JSON object per line)\n",
    "df = pd.read_json('covid_fact/COVIDFACT_dataset.jsonl', lines=True)\n",
    "\n",
    "# 2. Drop the unwanted columns (ignore if they’re missing)\n",
    "df = df.drop(columns=['flair', 'gold_source'], errors='ignore')\n",
    "\n",
    "# 3. Convert to a list of dicts\n",
    "records = df.to_dict(orient='records')\n",
    "\n",
    "# 4. (Optional) Write out to a normal JSON file\n",
    "with open('prepared_datasets/covid_fact.json', 'w') as f:\n",
    "    json.dump(records, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Prepare Politihop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_and_group(train_tsv, valid_tsv, test_tsv, output_path):\n",
    "    \"\"\"\n",
    "    1) Reads the three TSV splits.\n",
    "    2) Extracts only the truly validated evidence sentences (using `annotated_evidence`).\n",
    "    3) Concatenates each record’s evidence sentences into one string.\n",
    "    4) Groups by (statement, dataset), collecting those concatenated strings into lists.\n",
    "    5) Writes the grouped records to a single JSON file.\n",
    "    \"\"\"\n",
    "    def load_split(path, split_name, start_id, raw_recs):\n",
    "        df = pd.read_csv(path, sep='\\t', dtype=str)\n",
    "        uid = start_id\n",
    "        for _, row in df.iterrows():\n",
    "            # full ruling sentences\n",
    "            rulings = json.loads(row['ruling'])\n",
    "            # annotated_evidence: chain → [sentence IDs]\n",
    "            try:\n",
    "                chains = json.loads(row.get('annotated_evidence', '{}'))\n",
    "            except json.JSONDecodeError:\n",
    "                chains = {}\n",
    "            # gather all sentence IDs\n",
    "            ids = set()\n",
    "            for sid_list in chains.values():\n",
    "                for sid in sid_list:\n",
    "                    if isinstance(sid, str) and sid.isdigit():\n",
    "                        ids.add(int(sid))\n",
    "                    elif isinstance(sid, int):\n",
    "                        ids.add(sid)\n",
    "            # filter valid and sort\n",
    "            valid_ids = sorted(i for i in ids if 0 <= i < len(rulings))\n",
    "            # extract and concatenate\n",
    "            evidence_texts = [rulings[i] for i in valid_ids]\n",
    "            concatenated = \" \".join(evidence_texts).strip()\n",
    "            raw_recs.append({\n",
    "                'statement': row['statement'],\n",
    "                'politifact_label': row['politifact_label'],\n",
    "                'annotated_label': row['annotated_label'],\n",
    "                'dataset': split_name,\n",
    "                'evidence': concatenated\n",
    "            })\n",
    "            uid += 1\n",
    "        return uid\n",
    "\n",
    "    # step 1–3: extract and concatenate\n",
    "    raw_records = []\n",
    "    counter = 1\n",
    "    counter = load_split(train_tsv, 'train', counter, raw_records)\n",
    "    counter = load_split(valid_tsv, 'validation', counter, raw_records)\n",
    "    counter = load_split(test_tsv,  'test',       counter, raw_records)\n",
    "\n",
    "    # step 4: group by (statement, dataset)\n",
    "    grouped = {}\n",
    "    for rec in raw_records:\n",
    "        key = (rec['statement'], rec['dataset'])\n",
    "        if key not in grouped:\n",
    "            grouped[key] = {\n",
    "                'politifact_label': rec['politifact_label'],\n",
    "                'annotated_label': rec['annotated_label'],\n",
    "                'evidence': []\n",
    "            }\n",
    "        grouped[key]['evidence'].append(rec['evidence'])\n",
    "\n",
    "    # step 5: build the grouped list and write out\n",
    "    grouped_records = []\n",
    "    for (statement, dataset), data in grouped.items():\n",
    "        grouped_records.append({\n",
    "            'statement': statement,\n",
    "            'politifact_label': data['politifact_label'],\n",
    "            'annotated_label': data['annotated_label'],\n",
    "            'dataset':   dataset,\n",
    "            'evidence':  data['evidence']\n",
    "        })\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(grouped_records, f, indent=2)\n",
    "    print(f\"Wrote {len(grouped_records)} grouped records to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 497 grouped records to prepared_datasets/politihop_combined_and_grouped_ruling.json\n"
     ]
    }
   ],
   "source": [
    "extract_and_group(\n",
    "    train_tsv='politi_hop/politihop_train.tsv',\n",
    "    valid_tsv='politi_hop/politihop_valid.tsv',\n",
    "    test_tsv= 'politi_hop/politihop_test.tsv',\n",
    "    output_path='prepared_datasets/politihop_combined_and_grouped_ruling.json'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. prepare HoVer Dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def preprocess_dataset(input_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Reads a JSON file containing dataset examples, extracts claim, matching evidences,\n",
    "    and label for each entry, then writes the results as a JSON list.\n",
    "\n",
    "    Each output entry will have:\n",
    "      - 'claim': the claim text\n",
    "      - 'evidences': list of context contents whose titles appear in supporting_facts\n",
    "      - 'label': the example label (e.g., SUPPORTED, REFUTED)\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed = []\n",
    "    for example in data:\n",
    "        claim = example.get('claim', '')\n",
    "        label = example.get('label', '')\n",
    "\n",
    "        # Build a map from context title to its content\n",
    "        context = example.get('context', [])  # list of [title, content]\n",
    "        context_map = {title: content for title, content in context}\n",
    "\n",
    "        # Extract evidences by matching titles in supporting_facts\n",
    "        evidences = []\n",
    "        for title, _ in example.get('supporting_facts', []):\n",
    "            content = context_map.get(title)\n",
    "            if content:\n",
    "                evidences.append(content)\n",
    "\n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_evidences = []\n",
    "        for ev in evidences:\n",
    "            if ev not in seen:\n",
    "                seen.add(ev)\n",
    "                unique_evidences.append(ev)\n",
    "\n",
    "        processed.append({\n",
    "            'claim': claim,\n",
    "            'evidences': unique_evidences,\n",
    "            'label': label\n",
    "        })\n",
    "\n",
    "    # Write the processed list to output JSON file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess_dataset('hover/data/hover/doc_retrieval/hover_dev_doc_retrieval.json', 'prepared_datasets/hover_dev_full_docs.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dataset('hover/data/hover/doc_retrieval/hover_train_doc_retrieval.json', 'prepared_datasets/hover_train_full_docs.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
