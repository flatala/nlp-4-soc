{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1dafa1f",
   "metadata": {},
   "source": [
    "# G-EVAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6309f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import asyncio\n",
    "from faithfulness_eval_utils.geval import evaluate_geval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526454cc",
   "metadata": {},
   "source": [
    "## Example loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7da71211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 items to `output/geval/results.json`\n",
      "Averages written to `output/geval/stats.json`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'statement': \"Shakespeare wrote 'Romeo and Juliet'.\",\n",
       "   'explanation': \"Historical records and publication attributions confirm that Shakespeare wrote 'Romeo and Juliet'.\",\n",
       "   'evidences': [\"William Shakespeare is credited as the author of the play 'Romeo and Juliet'.\",\n",
       "    \"'Romeo and Juliet' was first published in 1597 and is attributed to Shakespeare.\"],\n",
       "   'label': 'SUPPORTED',\n",
       "   'q1_factual_claims_not_in_evidence': '**No**, the explanation does not make any factual claims not found in the evidence.',\n",
       "   'q2_reflects_key_points': '**Yes**, the explanation faithfully reflects the key points in the evidence, specifically highlighting the historical records and publication attributions that support Shakespeare\\'s authorship of \"Romeo and Juliet\".',\n",
       "   'faithfulness_score_0_5': 5.0,\n",
       "   'justification': '',\n",
       "   'raw_response': 'Here are my evaluations:\\n\\n1. **No**, the explanation does not make any factual claims not found in the evidence.\\n\\n2. **Yes**, the explanation faithfully reflects the key points in the evidence, specifically highlighting the historical records and publication attributions that support Shakespeare\\'s authorship of \"Romeo and Juliet\".\\n\\n3. Faithfulness Score: **5/5**\\n\\nJustification: The explanation only uses information present in the evidence, accurately summarizing the supporting points (historical records and publication attributions) without adding any unsupported claims or hallucinations.'},\n",
       "  {'statement': 'Eating carrots improves eyesight.',\n",
       "   'explanation': 'While carrots provide vitamin A, there is no evidence they improve eyesight beyond maintaining normal vision.',\n",
       "   'evidences': ['Carrots contain beta-carotene, which the body converts to vitamin A, a nutrient important for vision.',\n",
       "    'No clinical evidence shows that eating carrots prevents or cures vision problems beyond normal health benefits.'],\n",
       "   'label': 'NOT_SUPPORTED',\n",
       "   'q1_factual_claims_not_in_evidence': \"**No**: The explanation only summarizes the key points from the evidence, without making any additional claims that aren't supported by the provided data.\",\n",
       "   'q2_reflects_key_points': '**Yes**: The explanation accurately reflects the main points from the evidence, including the role of beta-carotene in vitamin A production and the lack of clinical evidence supporting improved eyesight beyond normal vision maintenance.',\n",
       "   'faithfulness_score_0_5': 5.0,\n",
       "   'justification': '',\n",
       "   'raw_response': \"Here are my evaluations:\\n\\n1. **No**: The explanation only summarizes the key points from the evidence, without making any additional claims that aren't supported by the provided data.\\n\\n2. **Yes**: The explanation accurately reflects the main points from the evidence, including the role of beta-carotene in vitamin A production and the lack of clinical evidence supporting improved eyesight beyond normal vision maintenance.\\n\\n3. Faithfulness Score: **5/5**\\nJustification: The explanation is entirely faithful to the evidence, without adding any unsupported claims or interpretations that aren't justified by the provided data. It accurately summarizes the key points from the evidence, making it a highly faithful representation of the original information.\"},\n",
       "  {'statement': 'Mount Everest is over 8,000 meters tall.',\n",
       "   'explanation': \"Multiple measurements, historical and modern, consistently report Everest's height above 8,000 meters.\",\n",
       "   'evidences': [\"Mount Everest's height was first measured in 1856 as 8,840 meters.\",\n",
       "    \"Recent GPS surveys have confirmed Everest's elevation at approximately 8,848.86 meters above sea level.\",\n",
       "    'Everest is the tallest mountain on Earth by elevation above sea level.'],\n",
       "   'label': 'SUPPORTED',\n",
       "   'q1_factual_claims_not_in_evidence': '**No**, the explanation does not make any factual claims not found in the evidence.',\n",
       "   'q2_reflects_key_points': \"**Yes**, the explanation faithfully reflects the key points in the evidence, specifically mentioning multiple measurements and reporting Everest's height above 8,000 meters, which is supported by the provided data.\",\n",
       "   'faithfulness_score_0_5': 5.0,\n",
       "   'justification': '',\n",
       "   'raw_response': \"Here are my evaluations:\\n\\n1. **No**, the explanation does not make any factual claims not found in the evidence.\\n\\n2. **Yes**, the explanation faithfully reflects the key points in the evidence, specifically mentioning multiple measurements and reporting Everest's height above 8,000 meters, which is supported by the provided data.\\n\\n3. Faithfulness Score: **5/5**\\nJustification: The explanation only uses information present in the evidence, accurately summarizing the historical measurement and recent GPS surveys that confirm Everest's elevation above 8,000 meters.\"},\n",
       "  {'statement': 'Honey never spoils.',\n",
       "   'explanation': 'Due to its chemical properties, honey resists spoilage and archaeological examples show it remains edible for millennia.',\n",
       "   'evidences': ['Archaeologists have found pots of honey in ancient Egyptian tombs over 3,000 years old that were still edible.',\n",
       "    \"Honey's low moisture and high acidity inhibit bacterial growth, allowing it to remain unspoiled.\"],\n",
       "   'label': 'SUPPORTED',\n",
       "   'q1_factual_claims_not_in_evidence': '**No**, the explanation does not make any factual claims not found in the evidence.',\n",
       "   'q2_reflects_key_points': \"**Yes**, the explanation faithfully reflects the key points in the evidence, specifically highlighting honey's chemical properties and its resistance to spoilage, which is supported by both archaeological examples and scientific explanations (low moisture and high acidity).\",\n",
       "   'faithfulness_score_0_5': 5.0,\n",
       "   'justification': '',\n",
       "   'raw_response': \"Here are my evaluations:\\n\\n1. **No**, the explanation does not make any factual claims not found in the evidence.\\n\\n2. **Yes**, the explanation faithfully reflects the key points in the evidence, specifically highlighting honey's chemical properties and its resistance to spoilage, which is supported by both archaeological examples and scientific explanations (low moisture and high acidity).\\n\\n3. Faithfulness Score: **5/5**\\n\\nJustification: The explanation only uses information present in the evidence, accurately summarizing the supporting points without adding any unsupported claims or hallucinations. It provides a clear and concise reflection of the key ideas presented in the evidence.\"},\n",
       "  {'statement': 'Honey never spoils.',\n",
       "   'explanation': 'Honey spoils after a week.',\n",
       "   'evidences': ['Archaeologists have found pots of honey in ancient Egyptian tombs over 3,000 years old that were still edible.',\n",
       "    \"Honey's low moisture and high acidity inhibit bacterial growth, allowing it to remain unspoiled.\"],\n",
       "   'label': 'NOT_SUPPORTED',\n",
       "   'q1_factual_claims_not_in_evidence': '**Yes**, the explanation makes a factual claim (\"Honey spoils after a week.\") that is not found in the evidence.',\n",
       "   'q2_reflects_key_points': '**No**, the explanation does not faithfully reflect the key points in the evidence, which suggest that honey can remain unspoiled for a long time due to its low moisture and high acidity.',\n",
       "   'faithfulness_score_0_5': 0.0,\n",
       "   'justification': '',\n",
       "   'raw_response': 'Here are my evaluations:\\n\\n1. **Yes**, the explanation makes a factual claim (\"Honey spoils after a week.\") that is not found in the evidence.\\n2. **No**, the explanation does not faithfully reflect the key points in the evidence, which suggest that honey can remain unspoiled for a long time due to its low moisture and high acidity.\\n3. Faithfulness Score: **0/5**. The explanation directly contradicts the evidence and makes an unsupported claim about honey spoiling after a week.\\n\\nJustification: The explanation is not faithful because it introduces a new, unsubstantiated claim that is opposite of what the evidence suggests.'}],\n",
       " {'avg_geval_score_0_5': 4.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../sample_complex.json\") as fin:\n",
    "    data = json.load(fin)\n",
    "\n",
    "await evaluate_geval(\n",
    "    data=data,\n",
    "    result_dir=\"output/geval\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36351a67",
   "metadata": {},
   "source": [
    "## Evaluating all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2aec05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57ce53923ca42d3aaffb70abfec66a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing deepseek_r1_32b_cot:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ..\\outputs\\converted_outputs\\deepseek_r1_32b_cot\\covid_fact_depseek_r1_cot.json\n",
      "Saved 1017 items to `output\\geval\\deepseek_r1_32b_cot/results.json`\n",
      "Averages written to `output\\geval\\deepseek_r1_32b_cot/stats.json`\n",
      "Finished evaluating ..\\outputs\\converted_outputs\\deepseek_r1_32b_cot\\covid_fact_depseek_r1_cot.json\n",
      "Evaluating ..\\outputs\\converted_outputs\\deepseek_r1_32b_cot\\hover_train_depseek-r1-cot.json\n",
      "Saved 2076 items to `output\\geval\\deepseek_r1_32b_cot/results.json`\n",
      "Averages written to `output\\geval\\deepseek_r1_32b_cot/stats.json`\n",
      "Finished evaluating ..\\outputs\\converted_outputs\\deepseek_r1_32b_cot\\hover_train_depseek-r1-cot.json\n",
      "Evaluating ..\\outputs\\converted_outputs\\deepseek_r1_32b_cot\\politi_hop_depseek_r1_cot.json\n",
      "Saved 497 items to `output\\geval\\deepseek_r1_32b_cot/results.json`\n",
      "Averages written to `output\\geval\\deepseek_r1_32b_cot/stats.json`\n",
      "Finished evaluating ..\\outputs\\converted_outputs\\deepseek_r1_32b_cot\\politi_hop_depseek_r1_cot.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bfa2378324417b9c55edd2f324c0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing gpt4o_cot:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ..\\outputs\\converted_outputs\\gpt4o_cot\\covid_fact_gpt4o_cot.json\n",
      "Saved 1050 items to `output\\geval\\gpt4o_cot/results.json`\n",
      "Averages written to `output\\geval\\gpt4o_cot/stats.json`\n",
      "Finished evaluating ..\\outputs\\converted_outputs\\gpt4o_cot\\covid_fact_gpt4o_cot.json\n",
      "Evaluating ..\\outputs\\converted_outputs\\gpt4o_cot\\hover_train_gpt4o_cot.json\n",
      "Saved 2050 items to `output\\geval\\gpt4o_cot/results.json`\n",
      "Averages written to `output\\geval\\gpt4o_cot/stats.json`\n",
      "Finished evaluating ..\\outputs\\converted_outputs\\gpt4o_cot\\hover_train_gpt4o_cot.json\n",
      "Evaluating ..\\outputs\\converted_outputs\\gpt4o_cot\\politi_hop_gpt4o_cot.json\n",
      "Saved 497 items to `output\\geval\\gpt4o_cot/results.json`\n",
      "Averages written to `output\\geval\\gpt4o_cot/stats.json`\n",
      "Finished evaluating ..\\outputs\\converted_outputs\\gpt4o_cot\\politi_hop_gpt4o_cot.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d69608bf01f4834928f2b7dc3423e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing gpt4o_non_cot:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ..\\outputs\\converted_outputs\\gpt4o_non_cot\\covid_fact_gpt4o_non_cot.json\n",
      "Saved 1050 items to `output\\geval\\gpt4o_non_cot/results.json`\n",
      "Averages written to `output\\geval\\gpt4o_non_cot/stats.json`\n",
      "Finished evaluating ..\\outputs\\converted_outputs\\gpt4o_non_cot\\covid_fact_gpt4o_non_cot.json\n",
      "Evaluating ..\\outputs\\converted_outputs\\gpt4o_non_cot\\hover_train_gpt4o_non_cot.json\n",
      "Saved 2050 items to `output\\geval\\gpt4o_non_cot/results.json`\n",
      "Averages written to `output\\geval\\gpt4o_non_cot/stats.json`\n",
      "Finished evaluating ..\\outputs\\converted_outputs\\gpt4o_non_cot\\hover_train_gpt4o_non_cot.json\n",
      "Evaluating ..\\outputs\\converted_outputs\\gpt4o_non_cot\\politi_hop_gpt4o_non_cot.json\n",
      "Saved 497 items to `output\\geval\\gpt4o_non_cot/results.json`\n",
      "Averages written to `output\\geval\\gpt4o_non_cot/stats.json`\n",
      "Finished evaluating ..\\outputs\\converted_outputs\\gpt4o_non_cot\\politi_hop_gpt4o_non_cot.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a80128902c4bf9b41cc63a2bc4b499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing mistral_7b_NO_cot:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ..\\outputs\\converted_outputs\\mistral_7b_NO_cot\\covid_fact_mistral_7b_no_cot.json\n",
      "Saved 1212 items to `output\\geval\\mistral_7b_NO_cot/results.json`\n",
      "Averages written to `output\\geval\\mistral_7b_NO_cot/stats.json`\n",
      "Finished evaluating ..\\outputs\\converted_outputs\\mistral_7b_NO_cot\\covid_fact_mistral_7b_no_cot.json\n",
      "Evaluating ..\\outputs\\converted_outputs\\mistral_7b_NO_cot\\hover_train_mistral_7b_no_cot.json\n"
     ]
    }
   ],
   "source": [
    "SKIP_EXISTING_RESULTS = True\n",
    "\n",
    "data_path = os.path.normpath(\"../inference_outputs/converted_outputs/\")\n",
    "model_setup_dirs = os.listdir(data_path)\n",
    "\n",
    "for model_setup_dir in model_setup_dirs:\n",
    "    if not os.path.isdir(os.path.join(data_path, model_setup_dir)):\n",
    "        continue\n",
    "    dataset_files = os.listdir(os.path.join(data_path, model_setup_dir))\n",
    "    if not dataset_files:\n",
    "        print(f\"No files found in {model_setup_dir}...\")\n",
    "        raise ValueError()\n",
    "\n",
    "    for dataset_file in tqdm(dataset_files, desc=f\"Processing {model_setup_dir}\"):\n",
    "        if not dataset_file.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        dataset_path = os.path.join(data_path, model_setup_dir, dataset_file)\n",
    "        result_dir = os.path.join(\"output\", \"geval\", model_setup_dir)\n",
    "\n",
    "        if SKIP_EXISTING_RESULTS and os.path.exists(result_dir):\n",
    "            print(\n",
    "                f\"Skipping {dataset_path} as results already exist in {result_dir}. Please check manually if results are valid.\"\n",
    "            )\n",
    "            if input(\"Do you want to continue? (y/n): \").strip().lower() != \"y\":\n",
    "                print(\"skipping...\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Evaluating {dataset_path}\")\n",
    "        with open(dataset_path, encoding=\"utf-8\") as fin:\n",
    "            data = json.load(fin)\n",
    "\n",
    "        await evaluate_geval(\n",
    "            data=data,\n",
    "            result_dir=result_dir,\n",
    "            stats_filename=f\"{dataset_file.replace('.json', '.stats.json')}\",\n",
    "            results_filename=f\"{dataset_file.replace('.json', '.eval.json')}\",\n",
    "        )\n",
    "        print(f\"Finished evaluating {dataset_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
