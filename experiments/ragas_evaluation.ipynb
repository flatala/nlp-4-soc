{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4356b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from time import time\n",
    "from faithfulness_eval_utils.ragas_eval import evaluate_ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e2062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file: ../inference/converted_outputs/gpt4o_non_cot/covid_fact_gpt4o_non_cot.json (covid_fact files are already done)\n",
      "Processing file: ../inference/converted_outputs/gpt4o_non_cot/hover_train_gpt4o_non_cot.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Scoring failed: The output parser failed to parse the output including retries.\n",
      "⚠️ Scoring failed: Failed to parse StringIO from completion {\"statements\": [{\"statement\": \"Hoch wie nie was published by a blues singer.\", \"reason\": \"The context does not mention Hoch wie nie being published by a blues singer. The album 'Hoch wie nie' is described as a posthumously-published 'Best of' double album by Austrian musician Falco, which contradicts the statement.\", \"verdict\": 0}, {\"statement\": \"The blues singer is also a composer.\", \"reason\": \"There is no information in the context about a blues singer being a composer. The context only mentions composers Jem Finer and Falco.\", \"verdict\": 0}, {\"statement\": \"The composer is a banjo player in The Pogues band.\", \"reason\": \"Jem Finer is mentioned as a banjo player for The Pogues, but there is no connection made between him and the composer of Hoch wie nie. Therefore, it cannot be directly inferred that the composer is a banjo player in The Pogues band.\", \"verdict\": 0}, {\"statement\": \"Falco is an Austrian musician.\", \"reason\": \"The context explicitly states that Falco is an Austrian musician, so this statement can be directly inferred.\", \"verdict\": 1}, {\"statement\": \"Jem Finer is a banjo player and composer for The Pogues.\", \"reason\": \"The context confirms Jem Finer as a banjo player and composer for The Pogues, so this statement can be directly inferred.\", \"verdict\": 1}, {\"statement\": \"No connection exists between Hoch wie nie, Falco, or Jem Finer of The Pogues.\", \"reason\": \"The context does not provide any information that would suggest a connection between these entities. Therefore, it is reasonable to conclude that no connection exists.\", \"verdict\": 1}, {\"statement\": \"No evidence supports them being the same individual or related to this claim.\", \"reason\": \"There is no information in the context that suggests Jem Finer and Falco are the same individual or related. Therefore, it can be concluded that there is no evidence supporting this claim.\", \"verdict\": 1}]}. Got: 1 validation error for StringIO\n",
      "text\n",
      "  Field required [type=missing, input_value={'statements': [{'stateme...claim.', 'verdict': 1}]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Failed to parse NLIStatementOutput from completion null. Got: 1 validation error for NLIStatementOutput\n",
      "  Input should be a valid dictionary or instance of NLIStatementOutput [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Scoring failed: The output parser failed to parse the output including retries.\n",
      "⚠️ Scoring failed: Invalid json output: The statements are:\n",
      "\n",
      "[\"The evidence provides information about Mermaid Got Married.\",\n",
      "\"Mermaid Got Married was based on the 1984 American film \"Splash\".\",\n",
      "\"The answer does not compare the number of writers between Christopher Robin and Splash.\",\n",
      "\"Neither Christopher Robin nor Splash mentions the number of writers.\" ]\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Failed to parse StringIO from completion {\"statements\": [{\"statement\": \"The evidence confirms that the Seacoast Region includes the cities of Dover and Portsmouth, New Hampshire.\", \"reason\": \"There is no evidence in the context to confirm this statement. The context only mentions that Dover is a city in Strafford County, New Hampshire, and Portsmouth is mentioned as the cultural and commercial hub of the region, but there is no mention of them being part of the same region.\", \"verdict\": 0}, {\"statement\": \"Dover is described as being in the New Hampshire Seacoast Region.\", \"reason\": \"The context explicitly states that Dover is a city in Strafford County, New Hampshire, and also mentions that it is located in the New Hampshire Seacoast region. This statement can be directly inferred from the context.\", \"verdict\": 1}, {\"statement\": \"Portsmouth is noted as the cultural and commercial hub of the New Hampshire Seacoast Region.\", \"reason\": \"The context explicitly states that Portsmouth serves as the cultural and commercial hub of the region, which confirms this statement.\", \"verdict\": 1}]}. Got: 1 validation error for StringIO\n",
      "text\n",
      "  Field required [type=missing, input_value={'statements': [{'stateme...ement.', 'verdict': 1}]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"properties\": {\"text\": {\"title\": \"Text\", \"type\": \"string\"}}, \"required\": [\"text\"], \"title\": \"StringIO\", \"type\": \"object\"}Do not use single quotes in your response but double quotes, properly escaped with a backslash.\n",
      "\n",
      "Now perform the same with the following input\n",
      "input: {\n",
      "    \"output_string\": \"Here is the output:\\n\\n{\\n\\\"statements\\\": [\\n    {\\n        \\\"statement\\\": \\\"Nicholas Boshier starred in Soul Mates alongside Christiaan Van Vuuren.\\\",\\n        \\\"reason\\\": \\\"The context explicitly states that Nicholas Boshier starred alongside Christiaan Van Vuuren in the series 'Soul Mates'.\\\",\\n        \\\"verdict\\\": 1\\n    },\\n    {\\n        \\\"statement\\\": \\\"Christiaan Van Vuuren developed the cartoon 'Beached Az'.\\\",\\n        \\\"reason\\\": \\\"The context mentions that Nicholas Boshier, along with Macfarlane and Jarod Green, developed the cartoon 'Beached Az', but it does not specify who specifically developed it. Therefore, it cannot be directly inferred that Christiaan Van Vuuren developed it.\\\",\\n        \\\"verdict\\\": 0\\n    },\\n    {\\n        \\\"'Beached Az' aired on Australian Broadcasting Corporation (ABC).\\\",\\n        \\\"reason\\\": \\\"The context states that 'Soul Mates', which stars Nicholas Boshier and Christiaan Van Vuuren, was screened on ABC2. However, it does not explicitly state that 'Beached Az' aired on ABC. Therefore, it cannot be directly inferred.\\\",\\n        \\\"verdict\\\": 0\\n    }\\n]\\n}\",\n",
      "    \"prompt_value\": \"Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"$defs\": {\"StatementFaithfulnessAnswer\": {\"properties\": {\"statement\": {\"description\": \"the original statement, word-by-word\", \"title\": \"Statement\", \"type\": \"string\"}, \"reason\": {\"description\": \"the reason of the verdict\", \"title\": \"Reason\", \"type\": \"string\"}, \"verdict\": {\"description\": \"the verdict(0/1) of the faithfulness.\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"], \"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\"}}, \"properties\": {\"statements\": {\"items\": {\"$ref\": \"#$defs/StatementFaithfulnessAnswer\"}}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"NLIStatementOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes, properly escaped with a backslash.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Failed to parse StringIO from completion {\"statements\": [{\"statement\": \"Greek Fire originated in the Byzantine Empire.\", \"reason\": \"The context does not mention Greek Fire or its origin. The statement cannot be directly inferred from the given context.\", \"verdict\": 0}, {\"statement\": \"The Byzantine Empire is specifically located in Constantinople.\", \"reason\": \"The context mentions that Constantinople Records was created by Billy Corgan, and it also mentions that the US Patent Office's website listed the label as being trademarked by Corgan. This implies that Constantinople refers to a location or entity associated with Billy Corgan, not necessarily the Byzantine Empire.\", \"verdict\": 0}, {\"statement\": \"Constantinople is geographically north of Chicago, Illinois.\", \"reason\": \"The context does not provide any information about the geographical location of Constantinople relative to Chicago. The statement cannot be directly inferred from the given context.\", \"verdict\": 0}, {\"statement\": \"Smashing Pumpkins formed in Chicago, Illinois.\", \"reason\": \"The context explicitly states that Smashing Pumpkins was formed by Billy Corgan and guitarist James Iha in Chicago, Illinois. This information can be directly inferred from the given context.\", \"verdict\": 1}, {\"statement\": \"Billy Corgan is associated with Constantinople Records.\", \"reason\": \"The context explicitly states that Constantinople Records is an upstart record label created by Billy Corgan of The Smashing Pumpkins. This information can be directly inferred from the given context.\", \"verdict\": 1}, {\"statement\": \"This aligns with the claim that Greek Fire was not originated in a more southern location than Smashing Pumpkins.\", \"reason\": \"The statement is unclear and does not provide any specific information about Greek Fire or its origin. The statement cannot be directly inferred from the given context.\", \"verdict\": 0}]}. Got: 1 validation error for StringIO\n",
      "text\n",
      "  Field required [type=missing, input_value={'statements': [{'stateme...ntext.', 'verdict': 0}]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Scoring failed: The output parser failed to parse the output including retries.\n",
      "⚠️ Scoring failed: Failed to parse NLIStatementOutput from completion {\"statements\": [{\"statement\": \"Operation Cold Comfort was a failed raid.\", \"reason\": \"The context explicitly states that Operation Cold Comfort was a failed SAS raid, so it can be directly inferred from the context.\", \"verdict\": 1}, {\"statement\": \"The operation was conducted by the SAS during World War II.\", \"reason\": \"The context mentions that the operation was an SAS raid during World War II, which confirms this statement.\", \"verdict\": 1}, {\"statement\": \"The SAS is a special forces unit.\", \"reason\": \"There is no information in the context that contradicts or supports the statement about the SAS being a special forces unit. However, it can be indirectly inferred from the context since the operation was conducted by the SAS during World War II.\", \"verdict\": 1}, {\"statement\": \"There is no evidence provided to verify that the SAS was founded in 1941.\"}]}. Got: 2 validation errors for NLIStatementOutput\n",
      "statements.3.reason\n",
      "  Field required [type=missing, input_value={'statement': 'There is n...S was founded in 1941.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "statements.3.verdict\n",
      "  Field required [type=missing, input_value={'statement': 'There is n...S was founded in 1941.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: Here is the output for the given input:\n",
      "\n",
      "{\n",
      "\"statements\": [\n",
      "\"The show Friends aired on NBC from September 22, 1994, to May 6, 2004.\",\n",
      "\"Friends aired on NBC from September 22, 1994, to May 6, 2004,\",\n",
      "\"The One with the Girl Who Hits Joey is the fifteenth episode of the fifth season.\",\n",
      "\"The claim that The One with the Girl Who Hits Joey is the fifteenth episode of Friends aligns with the evidence.\"]\n",
      "}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: Here is the output for the given input:\n",
      "\n",
      "{\n",
      "\"statements\": [\n",
      "\"The claim about Karl Attenberger working with Leni Riefenstahl on 'Triumph of the Will' and 'The End of America' being both made in the same year is false.\",\n",
      "'Triumph of the Will was made in 1935.',\n",
      "'Karl Attenberger worked with Leni Riefenstahl on Triumph of the Will.',\n",
      "'The evidence does not mention The End of America being made in 1935.'\n",
      "]\n",
      "}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: Here is the output for the given input:\n",
      "{\n",
      "\"statements\": [\n",
      "\"The series of books that Who Will Comfort Toffle? is a part of, released Moominvalley in November, came after something was published in 1965.\",\n",
      "'The series of books that Who Will Comfort Toffle? is a part of is the Moomin series by Tove Jansson.',\n",
      "'Who Will Comfort Toffle? was published in 1960 which predates 1965.',\n",
      "'Moominpappa at Sea is also part of the Moomin series by Tove Jansson.',\n",
      "'The loose reference to Hemingway's novel \"The Old Man and the Sea\" applies only to \"Moominpappa at Sea\" and not to \"Who Will Comfort Toffle?\".',\n",
      "\"The provided evidence does not support the overall claim.\"\n",
      "]\n",
      "}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"properties\": {\"text\": {\"title\": \"Text\", \"type\": \"string\"}}, \"required\": [\"text\"], \"title\": \"StringIO\", \"type\": \"object\"}Do not use single quotes in your response but double quotes, properly escaped with a backslash.\n",
      "\n",
      "Now perform the same with the following input\n",
      "input: {\n",
      "    \"output_string\": \"Here is the output:\\n\\n{\\n\\\"statements\\\": [\\n{\\n\\\"statement\\\": \\\"Mel Duncan holds a bachelor's degree in Political Science from Macalester College.\\\",\\n\\\"reason\\\": \\\"The context explicitly states that Melvin Earl Duncan holds a bachelor's degree in Political Science from Macalester College, so this statement can be directly inferred.\\\",\\n\\\"verdict\\\": 1\\n},\\n{\\n\\\"statement\\\": \\\"Not a history degree was held by Mel Duncan.\\\",\\n\\\"reason\\\": \\\"The context does not mention anything about Mel Duncan holding a history degree. In fact, it explicitly states that he holds a bachelor's degree in Political Science from Macalester College, so this statement can be directly inferred as false.\\\",\\n\\\"verdict\\\": 1\\n},\\n{\\n\\\"statement\\\": \\\"It is stated that Mel Duncan does not hold a history degree, but rather a Political Science degree from Macalester College.\\\"\\n\\\"reason\\\": \\\"This statement is essentially the same as the first one, which we already determined to be true. Therefore, this statement can also be directly inferred.\\\",\\n\\\"verdict\\\": 1\\n}\\n]\\n}\",\n",
      "    \"prompt_value\": \"Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"$defs\": {\"StatementFaithfulnessAnswer\": {\"properties\": {\"statement\": {\"description\": \"the original statement, word-by-word\", \"title\": \"Statement\", \"type\": \"string\"}, \"reason\": {\"description\": \"the reason of the verdict\", \"title\": \"Reason\", \"type\": \"string\"}, \"verdict\": {\"description\": \"the verdict(0/1) of the faithfulness.\"}, \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"], \"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\"}}, \"properties\": {\"statements\": {\"items\": {\"$ref\": \"#$defs/StatementFaithfulnessAnswer\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"NLIStatementOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes, properly escaped with a backslash.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n",
      "Please return the output in a JSON format that complies with the following schema:\n",
      "{\\\"$defs\\\": {\\\"StatementFaithfulnessAnswer\\\": {\\\"properties\\\": {\\\"statement\\\": {\\\"description\\\": \\\"the original statement, word-by-word\\\", \\\"title\\\": \\\"Statement\\\", \\\"type\\\": \\\"string\\\"}, \\\"reason\\\": {\\\"description\\\": \\\"the reason of the verdict\\\", \\\"title\\\": \\\"Reason\\\", \\\"type\\\": \\\"string\\\"}, \\\"verdict\\\": {\\\"description\\\": \\\"the verdict(0/1) of the faithfulness.\\\", \\\"title\\\": \\\"Verdict\\\", \\\"type\\\": \\\"integer\\\"}}, \\\"required\\\": [\\\"statement\\\", \\\"reason\\\", \\\"verdict\\\"], \\\"title\\\": \\\"StatementFaithfulnessAnswer\\\", \\\"type\\\": \\\"object\\\"}}, \\\"properties\\\": {\\\"statements\\\": {\\\"items\\\": {\\\"$ref\\\": \\\"#/$defs/StatementFaithfulnessAnswer\\\"}, \\\"title\\\": \\\"Statements\\\", \\\"type\\\": \\\"array\\\"}}, \\\"required\\\": [\\\"statements\\\"], \\\"title\\\": \\\"NLIStatementOutput\\\", \\\"type\\\": \\\"object\\\"}Do not use single quotes in your response but double quotes, properly escaped with a backslash.\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"context\": \"John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.\",\n",
      "    \"statements\": [\n",
      "        \"John is majoring in Biology.\",\n",
      "        \"John is taking a course on Artificial Intelligence.\",\n",
      "        \"John is a dedicated student.\",\n",
      "        \"John has a part-time job.\"\n",
      "    ]\n",
      "}\n",
      "Output: {\n",
      "    \"statements\": [\n",
      "        {\n",
      "            \"statement\": \"John is majoring in Biology.\",\n",
      "            \"reason\": \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\",\n",
      "            \"verdict\": 0\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"John is taking a course on Artificial Intelligence.\",\n",
      "            \"reason\": \"The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.\",\n",
      "            \"verdict\": 0\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"John is a dedicated student.\",\n",
      "            \"reason\": \"The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.\",\n",
      "            \"verdict\": 1\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"John has a part-time job.\",\n",
      "            \"reason\": \"There is no information given in the context about John having a part-time job.\",\n",
      "            \"verdict\": 0\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "Example 2\n",
      "Input: {\n",
      "    \"context\": \"Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.\",\n",
      "    \"statements\": [\n",
      "        \"Albert Einstein was a genius.\"\n",
      "    ]\n",
      "}\n",
      "Output: {\n",
      "    \"statements\": [\n",
      "        {\n",
      "            \"statement\": \"Albert Einstein was a genius.\",\n",
      "            \"reason\": \"The context and statement are unrelated\",\n",
      "            \"verdict\": 0\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "-----------------------------\n",
      "Now perform the same with the following input\n",
      "input: {\n",
      "    \"context\": \"Back to the Future: The Ride was a simulator ride at Universal Studios theme parks. It was based on and inspired by the \\\"Back to the Future\\\" film series and is a mini-sequel to 1990's \\\"Back to the Future Part III\\\". It was previously located at Universal Studios Florida and Universal Studios Hollywood, where it has since been replaced by \\\"The Simpsons Ride\\\", and at Universal Studios Japan where it has since been replaced by \\\"\",\n",
      "    \"statements\": [\n",
      "        \"The 'Back to the Future Ride' previously existed at Universal Studios Florida.\",\n",
      "        \"The 'Back to the Future Ride' has been replaced by 'The Simpsons Ride'.\",\n",
      "        \"'The Simpsons Ride' is adjacent to Springfield-themed areas at Universal Studios Hollywood.\",\n",
      "        \"There is no evidence indicating that Universal Studios Florida hosts both attractions concurrently or adjacent to Springfield-themed areas.\"\n",
      "    ]\n",
      "}\n",
      "Output: {\n",
      "    \"statements\": [\n",
      "        {\n",
      "            \"statement\": \"The 'Back to the Future Ride' previously existed at Universal Studios Florida.\",\n",
      "            \"reason\": \"According to the context, Back to the Future: The Ride was located at Universal Studios Florida and has since been replaced by The Simpsons Ride. Therefore, it can be directly inferred that the ride previously existed at this location.\",\n",
      "            \"verdict\": 1\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"The 'Back to the Future Ride' has been replaced by 'The Simpsons Ride'.\",\n",
      "            \"reason\": \"This statement is a direct quote from the context, which states that The Back to the Future Ride was replaced by The Simpsons Ride. Therefore, it can be directly inferred that this replacement occurred.\",\n",
      "            \"verdict\": 1\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"'The Simpsons Ride' is adjacent to Springfield-themed areas at Universal Studios Hollywood.\",\n",
      "            \"reason\": \"There is no information in the context about the location of The Simpsons Ride relative to Springfield-themed areas at Universal Studios Hollywood. Therefore, it cannot be directly inferred that this statement is true.\",\n",
      "            \"verdict\": 0\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"There is no evidence indicating that Universal Studios Florida hosts both attractions concurrently or adjacent to Springfield-themed areas.\",\n",
      "            \"reason\": \"This statement is a negation of the possibility of concurrent hosting or adjacency. Since there is no information in the context about this, it cannot be directly inferred that this statement is true.\",\n",
      "            \"verdict\": 0\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Failed to parse StringIO from completion {\"statements\": [{\"statement\": \"Rudolf H\\u00f6ss was the longest-serving resident-commandant of Auschwitz concentration camp.\", \"reason\": \"The context mentions that Kurt Gerstein supplied hydrogen cyanide (Zyklon B) to Rudolf H\\u00f6ss, and it also mentions that Rudolf H\\u00f6ss was the resident-commandant of Auschwitz. This information can be directly inferred from the context.\", \"verdict\": 1}, {\"statement\": \"Rudolf H\\u00f6ss conducted negotiations with the owners of Zyklon B.\", \"reason\": \"The context states that Kurt Gerstein, as Head of Technical Disinfection Services of the SS, supplied hydrogen cyanide (Zyklon B) to Rudolf H\\u00f6ss. This implies that Rudolf H\\u00f6ss was involved in the process of obtaining Zyklon B, which can be directly inferred from the context.\", \"verdict\": 1}, {\"statement\": \"Rainer Hoss is known to be Rudolf H\\u00f6ss's grandson.\", \"reason\": \"There is no information given in the context about Rainer Hoss or his relationship to Rudolf H\\u00f6ss. This statement cannot be directly inferred from the context.\", \"verdict\": 0}, {\"statement\": \"This supports the claim that Rainer Hoss's grandfather, Rudolf H\\u00f6ss, conducted negotiations with the owners and was the longest serving resident-commandant of Auschwitz concentration camp.\", \"reason\": \"The statement is trying to support a claim about Rainer Hoss's grandfather, but there is no information given in the context about Rainer Hoss or his family. This statement cannot be directly inferred from the context.\", \"verdict\": 0}]}. Got: 1 validation error for StringIO\n",
      "text\n",
      "  Field required [type=missing, input_value={'statements': [{'stateme...ntext.\", 'verdict': 0}]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Failed to parse NLIStatementOutput from completion {\"statements\": [{\"statement\": \"The evidence does not provide any information about an automobile driven by Garrett Smithley for MBM Motorsports.\", \"reason\": \"The context explicitly states that Garrett Smithley drives the No. 40 Toyota Camry for MBM Motorsports, which provides information about the automobile he drives for this team.\", \"verdict\": 0}, {\"statement\": \"The evidence provides no information about an automobile driven by Garrett Smithley for MBM Motorsports being sold in 1982.\", \"reason\": \"This statement is a specific instance of the previous one, and since the context does not provide any information about the sale of his vehicle in 1982, it can be directly inferred that this evidence does not provide such information.\", \"verdict\": 1}, {\"statement\": \"It only mentions the series and vehicles he drives currently.\"}]}. Got: 2 validation errors for NLIStatementOutput\n",
      "statements.2.reason\n",
      "  Field required [type=missing, input_value={'statement': 'It only me...s he drives currently.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "statements.2.verdict\n",
      "  Field required [type=missing, input_value={'statement': 'It only me...s he drives currently.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Scoring failed: The output parser failed to parse the output including retries.\n",
      "⚠️ Scoring failed: Failed to parse StringIO from completion {\"statements\": [{\"statement\": \"Peter Asher and Gordon, a duo managed by Norman Newell as an A&R manager for EMI, performed 'World Without Love'.\", \"reason\": \"The context mentions that Peter and Gordon were among the musicians Norman Newell worked with. Additionally, it is mentioned that Del Davis covered 'World Without Love', which was featured on 'Mellow Dubmarine'. This implies that Peter and Gordon performed the song.\", \"verdict\": 1}, {\"statement\": \"'World Without Love' was written by Peter Asher and Paul McCartney.\", \"reason\": \"The context does not provide information about who wrote 'World Without Love', so it cannot be directly inferred from the given context.\", \"verdict\": 0}, {\"statement\": \"Del Davis covered 'World Without Love', which was featured on 'Mellow Dubmarine'.\", \"reason\": \"This statement is a direct quote from the context, so it can be directly inferred.\", \"verdict\": 1}, {\"statement\": \"Peter Asher wrote the hit song of the duo ('World Without Love'), which was covered by Del Davis and featured on 'Mellow Dubmarine', while Norman Newell was the duo's A&R manager.\", \"reason\": \"The context does not provide information about who wrote the hit song, so it cannot be directly inferred from the given context. Additionally, there is no evidence that Peter Asher wrote the song or that Del Davis covered it.\", \"verdict\": 0}]}. Got: 1 validation error for StringIO\n",
      "text\n",
      "  Field required [type=missing, input_value={'statements': [{'stateme...ed it.', 'verdict': 0}]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Scoring failed: The output parser failed to parse the output including retries.\n",
      "⚠️ Scoring failed: Failed to parse StringIO from completion {\"statements\": [{\"statement\": \"The winner of The 1974 Pacific Coast Open singles tournament is Ross Case.\", \"reason\": \"The context states that the event was played at the Cow Palace in San Francisco, not New York. Additionally, it does not mention anything about the location being changed to New York.\", \"verdict\": 0}, {\"statement\": \"No information is provided about Ross Case's career-high singles ranking.\", \"reason\": \"The context only provides information about the tournament and its winner, but does not provide any details about Ross Case's career or his singles ranking.\", \"verdict\": 1}, {\"statement\": \"No evidence is provided about Slobodan \\u017divojinovi\\u0107's career-high singles ranking to compare with Ross Case's.\"}]}. Got: 1 validation error for StringIO\n",
      "text\n",
      "  Field required [type=missing, input_value={'statements': [{'stateme...re with Ross Case's.\"}]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"$defs\": {\"StatementFaithfulnessAnswer\": {\"properties\": {\"statement\": {\"description\": \"the original statement, word-by-word\", \"title\": \"Statement\", \"type\": \"string\"}, \"reason\": {\"description\": \"the reason of the verdict\", \"title\": \"Reason\", \"type\": \"string\"}, \"verdict\": {\"description\": \"the verdict(0/1) of the faithfulness.\", \"title\": \"Verdict\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"], \"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\"}}, \"properties\": {\"statements\": {\"items\": {\"$ref\": \"#/$defs/StatementFaithfulnessAnswer\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"NLIStatementOutput\", \"type\": \"object\"}\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"context\": \"John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.\",\n",
      "    \"statements\": [\n",
      "        \"John is majoring in Biology.\",\n",
      "        \"John is taking a course on Artificial Intelligence.\",\n",
      "        \"John is a dedicated student.\",\n",
      "        \"John has a part-time job.\"\n",
      "    ]\n",
      "}\n",
      "Output: {\n",
      "    \"statements\": [\n",
      "        {\n",
      "            \"statement\": \"John is majoring in Biology.\",\n",
      "            \"reason\": \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\",\n",
      "            \"verdict\": 0\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"John is taking a course on Artificial Intelligence.\",\n",
      "            \"reason\": \"The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.\",\n",
      "            \"verdict\": 0\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"John is a dedicated student.\",\n",
      "            \"reason\": \"The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.\",\n",
      "            \"verdict\": 1\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"John has a part-time job.\",\n",
      "            \"reason\": \"There is no information given in the context about John having a part-time job.\",\n",
      "            \"verdict\": 0\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "Example 2\n",
      "Input: {\n",
      "    \"context\": \"Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.\",\n",
      "    \"statements\": [\n",
      "        \"Albert Einstein was a genius.\"\n",
      "    ]\n",
      "}\n",
      "Output: {\n",
      "    \"statements\": [\n",
      "        {\n",
      "            \"statement\": \"Albert Einstein was a genius.\",\n",
      "            \"reason\": \"The context and statement are unrelated\",\n",
      "            \"verdict\": 0\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "-----------------------------\n",
      "\n",
      "Now perform the same with the following input\n",
      "input: {\n",
      "    \"context\": \"Flanary Archeological Site is a historic archaeological site located near Dungannon in Scott County, Virginia, United States. Located across the Clinch River from Dungannon, the site was inhabited as early as 6000 BC and remained in periodic use into the Woodland period, with occupation potentially continuing until c. AD 1600. The terminus ad quem for occupation is 1750, when Thomas Walker's expedition passed through the area and found no Indian villages. Excavations conducted in 1977 in preparation for the construction of a bridge revealed that the village site, featuring posthole patterns indicating a palisade surrounding the village, lay primarily south of the bridge in the vicinity of a 1764 log cabin.\\nThomas Walker (January 25, 1715 – November 9, 1794) was a distinguished physician and explorer from Virginia; in the mid-18th century, he was part of an expedition to the region beyond the Allegheny Mountains and the unsettled area of British North America. Walker and fellow Virginian, Indian agent, explorer for Patrick Henry, legislator of three states, surveyor of KY/VA & TN/NC borders, and later Revolutionary war general, Joseph Martin, were some of the first colonialists to travel in this area. Martin's son, Revolutionary War officer Col. William Martin, describes the naming of the area and river in a letter to historian Lyman Draper,\\nDr. Thomas Walker State Historic Site is a park located six miles southeast of Barbourville in Knox County in the U.S. state of Kentucky. The land was donated by the American Legion and the people of Barbourville, and marks the area where Kentucky pioneer Thomas Walker, a physician, built his cabin in 1750. A representative cabin marks the spot of \\\"the first house in Kentucky\\\". The site was dedicated in 1931. A replica of the cabin can be toured.\",\n",
      "    \"statements\": [\n",
      "        \"A Virginian explorer passed through an area near Garden Mountain.\",\n",
      "        \"The explorer was Thomas Walker.\",\n",
      "        \"Thomas Walker built his cabin in an area that is now the Dr. Thomas Walker State Historic Site.\",\n",
      "        \"This area is located in Kentucky.\"\n",
      "    ]\n",
      "}\n",
      "Output: {\n",
      "    \"statements\": [\n",
      "        {\n",
      "            \"statement\": \"A Virginian explorer passed through an area near Garden Mountain.\",\n",
      "            \"reason\": \"The context mentions Thomas Walker, a Virginian explorer, who was part of an expedition to the region beyond the Allegheny Mountains and the unsettled area of British North America. Although it does not specifically mention Garden Mountain, it is possible that this could be the same area mentioned in the statement.\",\n",
      "            \"verdict\": 0\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"The explorer was Thomas Walker.\",\n",
      "            \"reason\": \"The context explicitly mentions Thomas Walker as a distinguished physician and explorer from Virginia, who was part of an expedition to the region beyond the Allegheny Mountains and the unsettled area of British North America.\",\n",
      "            \"verdict\": 1\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"Thomas Walker built his cabin in an area that is now the Dr. Thomas Walker State Historic Site.\",\n",
      "            \"reason\": \"The context states that Thomas Walker built his cabin in 1750, which is marked by a representative cabin at the Dr. Thomas Walker State Historic Site.\",\n",
      "            \"verdict\": 1\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"This area is located in Kentucky.\"\n",
      "\"reason\": \"The context states that the site was dedicated in 1931 and marks the area where Kentucky pioneer Thomas Walker, a physician, built his cabin in 1750. Additionally, it mentions that Dr. Thomas Walker State Historic Site is a park located six miles southeast of Barbourville in Knox County in the U.S. state of Kentucky.\",\n",
      "            \"verdict\": 1\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Failed to parse StringIO from completion {\"statements\": [{\"statement\": \"On August 5, 2011, Jean Claude Trichet was the former governor of the ECB.\", \"reason\": \"The context states that on August 5, 2011, Jean Claude Trichet wrote a 'secret' letter to Italian government. This implies he was still the former governor of ECB at that time.\", \"verdict\": 1}, {\"statement\": \"Jean Claude Trichet co-wrote a 'secret' letter to the Italian government on August 5, 2011.\", \"reason\": \"The context explicitly states that Jean Claude Trichet wrote a 'secret' letter to Italian government along with Mario Draghi on August 5, 2011. This can be directly inferred from the context.\", \"verdict\": 1}, {\"statement\": \"Mario Draghi served as Chairman of the Financial Stability Board until 2011.\", \"reason\": \"The context states that Mario Draghi previously served as Chairman of the Financial Stability Board from 2009 to 2011. This implies he served in this role until 2011.\", \"verdict\": 1}, {\"statement\": \"This directly supports the claim that a 'secret' letter was signed by Jean Claude Trichet and Mario Draghi on August 5, 2011.\", \"reason\": \"The context explicitly states that Jean Claude Trichet and Mario Draghi wrote a 'secret' letter to Italian government on August 5, 2011. This statement is directly supported by the context.\", \"verdict\": 1}]}. Got: 1 validation error for StringIO\n",
      "text\n",
      "  Field required [type=missing, input_value={'statements': [{'stateme...ntext.\", 'verdict': 1}]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Failed to parse StringIO from completion {\"statements\": [{\"statement\": \"Don Williams is a country music artist.\", \"reason\": \"The context states that Don Williams released an album called 'Traces' which includes the song 'Come from the Heart'. This information implies that Don Williams is a country music artist, as he is releasing a country music album.\", \"verdict\": 1}, {\"statement\": \"'Come from the Heart' was first recorded by Don Williams.\", \"reason\": \"The context states that 'Come from the Heart' was first recorded and released on the 1987 Don Williams album 'Traces'. This information directly supports the claim that the song was first recorded by Don Williams.\", \"verdict\": 1}, {\"statement\": \"The evidence states that 'Come from the Heart' was first recorded by an artist who sang country music supports the claim that the song was first recorded by an artist who sang country music.\", \"reason\": \"This statement is not a direct inference, but rather a rephrasing of the original context. The verdict should be 0.\", \"verdict\": 0}]}. Got: 1 validation error for StringIO\n",
      "text\n",
      "  Field required [type=missing, input_value={'statements': [{'stateme... be 0.', 'verdict': 0}]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: The evidence confirms that Jennifer Kendal starred in the 1970 film Bombay Talkie.\\nReason: There is no evidence in the context to confirm or deny this statement. The context only mentions Jennifer Kendal's film appearances, but does not specifically mention her starring in the 1970 film Bombay Talkie.\\nVerdict: 0\\n\n",
      "Jennifer Kendal was the mother of Karan Kapoor.\\nReason: The context explicitly states that Jennifer Kendal is the mother of Karan Kapoor. This information can be directly inferred from the given context.\\nVerdict: 1\\n\n",
      "This supports the claim that Jennifer Kendal starred in the 1970 film Bombay Talkie.\\nReason: The statement itself claims to support the fact that Jennifer Kendal starred in the 1970 film Bombay Talkie, but this is not supported by the given context. The context does mention Jennifer Kendal's film appearances, but does not specifically confirm her starring in the 1970 film Bombay Talkie.\\nVerdict: 0\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Failed to parse NLIStatementOutput from completion null. Got: 1 validation error for NLIStatementOutput\n",
      "  Input should be a valid dictionary or instance of NLIStatementOutput [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Failed to parse NLIStatementOutput from completion null. Got: 1 validation error for NLIStatementOutput\n",
      "  Input should be a valid dictionary or instance of NLIStatementOutput [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: {\"statements\": [{\"statement\": \"Ann Beattie has the profession of writer.\", \"reason\": \"The context explicitly states that Ann Beattie is an American novelist and short story writer, which implies she has the profession of a writer.\", \"verdict\": 1}, {\"statement\": \"Ann Beattie is an American novelist and short story writer.\", \"reason\": \"This statement can be directly inferred from the context as it provides Ann Beattie's profession explicitly.\", \"verdict\": 1}, {\"statement\": \"Elisabeth Vrba's colleague, Stephen Jay Gould, coined the word 'exaptation' alongside her.\", \"reason\": \"The context states that Elisabeth Vrba coined the word exaptation with Stephen Jay Gould, which confirms this statement.\", \"verdict\": 1}, {\"statement\": \"Stephen Jay Gould authored several books on evolutionary biology.\", \"reason\": \"There is no information in the context about Stephen Jay Gould's book authorship or topic. Therefore, it cannot be directly inferred that he authored several books on evolutionary biology.\", \"verdict\": 0}, {\"statement\": \"Stephen Jay Gould is a writer.\", \"reason\": \"The context does not provide any information about Stephen Jay Gould's profession as a writer. It only mentions his collaboration with Elisabeth Vrba in coining the word exaptation.\", \"verdict\": 0}, {\"statement\": \"Both Elisabeth Vrba's colleague and Ann Beattie share the profession of being writers.\"\n",
      "\"reason\": \"The context states that Ann Beattie is a writer, but it does not provide any information about Elisabeth Vrba's profession. Therefore, it cannot be directly inferred that they both share the profession of being writers.\",\n",
      "\"verdict\": 0\n",
      "},]}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: Here is the output for the given input:\n",
      "{\n",
      "\"statements\": [\n",
      "\"The evidence explicitly confirms that 'The End' is an EP by English heavy metal band Black Sabbath.\",\n",
      "'The End' is released on January 20, 2016,\n",
      "'The End' was only available at dates on their End Tour,\n",
      "their End Tour was their farewell tour\n",
      "]\n",
      "}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"properties\": {\"text\": {\"title\": \"Text\", \"type\": \"string\"}}, \"required\": [\"text\"], \"title\": \"StringIO\", \"type\": \"object\"}Do not use single quotes in your response but double quotes, properly escaped with a backslash.\n",
      "\n",
      "Now perform the same with the following input\n",
      "input: {\n",
      "    \"output_string\": \"Here is the output in JSON format:\\n\\n{\\n\\\"statements\\\": [\\n{\\n\\\"statement\\\": \\\"Douglas Richard Hofstadter's profession is the professor of cognitive science.\\\",\\n\\\"reason\\\": \\\"The context explicitly states that Douglas Richard Hofstadter is an American professor of cognitive science, which directly answers this statement.\\\",\\n\\\"verdict\\\": 1\\n},\\n{\\n\\\"statement\\\": \\\"The evidence explicitly states that Douglas Richard Hofstadter is an American professor of cognitive science.\\\",\\n\\\"reason\\\": \\\"This statement is a direct quote from the context, so it can be inferred as true.\\\",\\n\\\"verdict\\\": 1\\n},\\n{\\n\\\"statement\\\": \\\"The evidence supports the claim that Douglas Richard Hofstadter is a professor of cognitive science.\\\"\\n\\\"reason\\\": \\\"The context provides information about Hofstadter's research and profession, which directly supports this statement.\\\",\\n\\\"verdict\\\": 1\\n}\\n]\\n}\",\n",
      "    \"prompt_value\": \"Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"$defs\": {\"StatementFaithfulnessAnswer\": {\"properties\": {\"statement\": {\"description\": \"the original statement, word-by-word\", \"title\": \"Statement\", \"type\": \"string\"}, \"reason\": {\"description\": \"the reason of the verdict\", \"title\": \"Reason\", \"type\": \"string\"}, \"verdict\": {\"description\": \"the verdict(0/1) of the faithfulness.\"}, \"title\": \"Verdict\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"], \"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\"}}, \"properties\": {\"statements\": {\"items\": {\"$ref\": \"#$defs/StatementFaithfulnessAnswer\"}}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"NLIStatementOutput\", \"type\": \"object\"}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: {\"statements\": [{\"statement\": \"Brent Hodge co-directed 'Winning America'.\",\n",
      "\"reason\": \"The context explicitly states that Brent Hodge directed and produced 'Winning America', along with Thomas Buchan.\",\n",
      "\"verdict\": 1\n",
      "}, {\"statement\": \"This confirms Brent Hodge's identity as a director.\",\n",
      "\"reason\": \"The statement is not directly related to the provided context, which only discusses Brent Hodge's work and awards. Therefore, it cannot be inferred from the given information.\",\n",
      "\"verdict\": 0\n",
      "}, {\"statement\": \"'The Cost' was directed by Brent Hodge.\",\n",
      "\"reason\": \"There is no mention of 'The Cost' in the provided context, which only discusses Brent Hodge's work on documentaries such as 'I Am Chris Farley', 'A Brony Tale', and 'Winning America'. Therefore, it cannot be inferred from the given information.\",\n",
      "\"verdict\": 0\n",
      "}, {\"statement\": \"This affirms Brent Hodge's recognition as a director.\",\n",
      "\"reason\": \"The statement is not directly related to the provided context, which only discusses Brent Hodge's work and awards. Therefore, it cannot be inferred from the given information.\",\n",
      "\"verdict\": 0\n",
      "}, {\"statement\": \"The claim that Brent Hodge is a director aligns with the evidence provided.\"\n",
      "\"reason\": \"The context explicitly states that Brent Hodge directed several documentaries, including 'I Am Chris Farley', 'A Brony Tale', and 'Winning America'. This confirms his identity as a director.\",\n",
      "\"verdict\": 1\n",
      "}]}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: Here is the output for the given input:\n",
      "\n",
      "{\n",
      "\"statements\": [\n",
      "\"The film with Fortuosity as a song was created before the film Mars Needs Moms.\",\n",
      "\"Fortuosity is a song from the 1967 film The Happiest Millionaire,\",\n",
      "\"The 1967 film The Happiest Millionaire predates the 2011 release of Mars Needs Moms.\",\n",
      "\"The evidence indicates that the claim is supported by the evidence.\"\n",
      "]\n",
      "}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: Here is the output for the given input:\n",
      "\n",
      "{\n",
      "\"statements\": [\n",
      "\"The adult film with \"Fortuosity\" as the first song was created.\",\n",
      "\"The evidence confirms that \\\"Fortuosity\\\" is the first song in the 1967 film \\\"The Happiest Millionaire\\\".\",\n",
      "\"This film was created before 'Mars Needs Moms', which was released in 2011.\"\n",
      "]\n",
      "}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: Here is the output for the given input:\n",
      "\n",
      "{\n",
      "\"statements\": [\n",
      "\"The film with Fortuosity as the first song was created before Mars Needs Moms film.\",\n",
      "\"Fortuosity is the first song in The Happiest Millionaire film.\",\n",
      "\"The Happiest Millionaire film was released in 1967.\",\n",
      "\"Mars Needs Moms film was released in 2011.\",\n",
      "\"The Happiest Millionaire film was created before Mars Needs Moms film.\"\n",
      "]\n",
      "}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Failed to parse NLIStatementOutput from completion {\"statements\": [{\"statement\": \"The film with Fortuosity as the first song was created before another film.\", \"reason\": \"The context mentions that 'Fortuosity' is the first song in the 1967 motion picture The Happiest Millionaire, and Mars Needs Moms is a 2011 film. Therefore, it can be directly inferred that the film with Fortuosity as the first song was created before Mars Needs Moms.\", \"verdict\": 1}, {\"statement\": \"The other film was created by the studio ImageMovers.\", \"reason\": \"The context states that ImageMovers is an American independent film studio, and it produced Mars Needs Moms. Therefore, it can be directly inferred that the other film (presumably 'The Happiest Millionaire') was also created by ImageMovers.\", \"verdict\": 1}, {\"statement\": \"'Fortuosity' is the first song in 'The Happiest Millionaire', a film created in 1967.\", \"reason\": \"This statement is a direct quote from the context, so it can be directly inferred that 'Fortuosity' is indeed the first song in The Happiest Millionaire, and it was created in 1967.\", \"verdict\": 1}, {\"statement\": \"'The Happiest Millionaire' predates 'Mars Needs Moms', a film released in 2011.\"}, {\"reason\": \"The context states that 'The Happiest Millionaire' is a 1967 film, and Mars Needs Moms was released in 2011. Therefore, it can be directly inferred that The Happiest Millionaire predates Mars Needs Moms.\", \"verdict\": 1}]}. Got: 3 validation errors for NLIStatementOutput\n",
      "statements.3.reason\n",
      "  Field required [type=missing, input_value={'statement': \"'The Happi...film released in 2011.\"}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "statements.3.verdict\n",
      "  Field required [type=missing, input_value={'statement': \"'The Happi...film released in 2011.\"}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "statements.4.statement\n",
      "  Field required [type=missing, input_value={'reason': \"The context s...ds Moms.\", 'verdict': 1}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Failed to parse NLIStatementOutput from completion {}. Got: 1 validation error for NLIStatementOutput\n",
      "statements\n",
      "  Field required [type=missing, input_value={}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Failed to parse StringIO from completion null. Got: 1 validation error for StringIO\n",
      "  Input should be a valid dictionary or instance of StringIO [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "⚠️ Scoring failed: Invalid json output: Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement cannot be directly inferred based on the context.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"properties\": {\"text\": {\"title\": \"Text\", \"type\": \"string\"}}, \"required\": [\"text\"], \"title\": \"StringIO\", \"type\": \"object\"}Do not use single quotes in your response but double quotes, properly escaped with a backslash.\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"context\": \"John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.\",\n",
      "    \"statements\": [\n",
      "        \"John is majoring in Biology.\",\n",
      "        \"John is taking a course on Artificial Intelligence.\",\n",
      "        \"John is a dedicated student.\",\n",
      "        \"John has a part-time job.\"\\n    ]\\n}\n",
      "Output: {\n",
      "    \"statements\": [\n",
      "        {\n",
      "            \"statement\": \"John is majoring in Biology.\",\n",
      "            \"reason\": \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\",\n",
      "            \"verdict\": 0\\n        },\n",
      "        {\n",
      "            \"statement\": \"John is taking a course on Artificial Intelligence.\",\n",
      "            \"reason\": \"The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.\",\n",
      "            \"verdict\": 0\\n        },\n",
      "        {\n",
      "            \"statement\": \"John is a dedicated student.\",\n",
      "            \"reason\": \"The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.\",\n",
      "            \"verdict\": 1\\n        },\n",
      "        {\n",
      "            \"statement\": \"John has a part-time job.\",\n",
      "            \"reason\": \"There is no information given in the context about John having a part-time job.\",\n",
      "            \"verdict\": 0\\n        }\\n    ]\\n}\n",
      "\n",
      "Example 2\n",
      "Input: {\n",
      "    \"context\": \"Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.\",\n",
      "    \"statements\": [\n",
      "        \"Albert Einstein was a genius.\"\\n    ]\\n}\n",
      "Output: {\n",
      "    \"statements\": [\n",
      "        {\n",
      "            \"statement\": \"Albert Einstein was a genius.\",\n",
      "            \"reason\": \"The context and statement are unrelated\",\\n            \"verdict\": 0\\n        }\\n    ]\\n}\\n-----------------------------\\nNow perform the same with the following input\n",
      "input: {\n",
      "    \"context\": \"Old Ship African Methodist Episcopal Zion Church is a historic African Methodist Episcopal Zion Church in Montgomery, Alabama. It is the oldest African American church congregation in the city, established in 1852. The current Classical Revival-style building was designed by Jim Alexander and was completed in 1918. It is the fourth building the congregation has erected at this location. Scenes from the 1982 television movie, \\\"Sister, Sister\\\", were shot at the church. It was placed on the Alabama Register of Landmarks and Heritage on March 3, 1976 and the National Register of Historic Places on January 24, 1991.\",\n",
      "    \"statements\": [\n",
      "        \"The film The Omega Man featured Rosalind Cash.\",\\n        \"The 1982 television movie was shot at Old Ship African Methodist Episcopal Zion Church.\",\\n        \"The 1982 television movie is titled 'Sister, Sister'.\",\\n        \"There is no information about Rosalind Cash or the film 'The Omega Man' in the evidence.\",\\n        \"The claim lacks supporting evidence.\"\\n    ]\\n}\\nOutput: {\n",
      "    \"statements\": [\n",
      "        {\n",
      "            \"statement\": \"The film The Omega Man featured Rosalind Cash.\",\n",
      "            \"reason\": \"There is no information about 'The Omega Man' or Rosalind Cash in the context. Therefore, it cannot be deduced that the film featured Rosalind Cash.\",\\n            \"verdict\": 0\\n        },\\n        {\n",
      "            \"statement\": \"The 1982 television movie was shot at Old Ship African Methodist Episcopal Zion Church.\",\n",
      "            \"reason\": \"The context explicitly states that scenes from the 1982 television movie were shot at the church. Therefore, it can be directly inferred that the statement is true.\",\\n            \"verdict\": 1\\n        },\\n        {\n",
      "            \"statement\": \"The 1982 television movie is titled 'Sister, Sister'.\",\\n            \"reason\": \"The context explicitly states that scenes from the 1982 television movie were shot at the church and that it was titled 'Sister, Sister'. Therefore, it can be directly inferred that the statement is true.\",\\n            \"verdict\": 1\\n        },\\n        {\n",
      "            \"statement\": \"There is no information about Rosalind Cash or the film 'The Omega Man' in the evidence.\",\n",
      "            \"reason\": \"This statement is actually a claim being made about the context, not a statement to be judged. Therefore, it cannot be given a verdict of 0 or 1.\",\\n            \"verdict\": null\\n        },\\n        {\n",
      "            \"statement\": \"The claim lacks supporting evidence.\",\n",
      "            \"reason\": \"This statement is actually a claim being made about the context, not a statement to be judged. Therefore, it cannot be given a verdict of 0 or 1.\",\\n            \"verdict\": null\\n        }\\n    ]\\n}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Scoring failed: The output parser failed to parse the output including retries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Scoring failed: The output parser failed to parse the output including retries.\n",
      "Saved 2000 items to `output/ragas/gpt4o_non_cot/hover_train_gpt4o_non_cot.results.json`\n",
      "Averages written to `output/ragas/gpt4o_non_cot/hover_train_gpt4o_non_cot.stats.json`\n",
      "Processed hover_train_gpt4o_non_cot.json in 286.14 minutes. Total elapsed time: 286.14 minutes.\n",
      "Processing file: ../inference/converted_outputs/gpt4o_non_cot/politi_hop_gpt4o_non_cot.json...\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 206: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     t2 = time()\n\u001b[32m     21\u001b[39m     stats_output_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename.replace(\u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.stats.json\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.10-windows-x86_64-none\\Lib\\json\\__init__.py:293\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(fp, *, \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, object_hook=\u001b[38;5;28;01mNone\u001b[39;00m, parse_float=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    275\u001b[39m         parse_int=\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant=\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook=\u001b[38;5;28;01mNone\u001b[39;00m, **kw):\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    278\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    294\u001b[39m         \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28mcls\u001b[39m, object_hook=object_hook,\n\u001b[32m    295\u001b[39m         parse_float=parse_float, parse_int=parse_int,\n\u001b[32m    296\u001b[39m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.10-windows-x86_64-none\\Lib\\encodings\\cp1252.py:23\u001b[39m, in \u001b[36mIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs.charmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m.errors,decoding_table)[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'charmap' codec can't decode byte 0x9d in position 206: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "EVALUATOR_MODEL = \"llama3:8b\"\n",
    "\n",
    "MODEL = \"gpt4o_non_cot\"\n",
    "# MODEL CHOICES ['mistral_7b_cot', 'mistral_7b_non_cot', 'gpt4o_cot', 'gpt4o_non_cot', 'deepseek_r1_32b_cot']\n",
    "\n",
    "\n",
    "t1 = time()\n",
    "directory_path = f\"../inference/converted_outputs/{MODEL}/\"\n",
    "all_files = os.listdir(directory_path)\n",
    "json_files = [f for f in all_files if f.endswith(\".json\")]\n",
    "\n",
    "for filename in json_files:\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "    print(f\"Processing file: {file_path}...\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        data = json.load(fin)\n",
    "        t2 = time()\n",
    "        stats_output_filename = f\"{filename.replace('.json', '.stats.json')}\"\n",
    "        results_output_filename = f\"{filename.replace('.json', '.results.json')}\"\n",
    "\n",
    "        await evaluate_ragas(\n",
    "            data,\n",
    "            evaluator_model=EVALUATOR_MODEL,\n",
    "            results_filename=results_output_filename,\n",
    "            stats_filename=stats_output_filename,\n",
    "            result_dir=f\"output/ragas/{MODEL}\",\n",
    "        )\n",
    "        t3 = time()\n",
    "        print(\n",
    "            f\"Processed {filename} in {(t3 - t2) / 60:.2f} minutes. Total elapsed time: {(t3 - t1) / 60:.2f} minutes.\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
